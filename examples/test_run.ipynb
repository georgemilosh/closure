{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df742637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import src.trainers as tr\n",
    "import importlib as imp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5f56bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_features_targets_kwargs = {'fields_to_read' :    {\"B\": True,\"B_ext\": False,\"divB\": False,\"E\": True,\"E_ext\": False,\"rho\": True,\"J\": True,\n",
    "                                                      \"P\": True,\"PI\": True,\"Heat_flux\": False,\"N\": False,\"Qrem\": False}, # which fields to read\n",
    "                                'request_features' :  ['rho_e', 'Bx', 'By', 'Bz', 'Vx_e', 'Vy_e', 'Vz_e', 'Ex', 'Ey', 'Ez'],  # what network needs to make predictions\n",
    "                                'request_targets' :   [\"Pxx_e\", \"Pyy_e\",\"Pzz_e\",\"Pxy_e\",\"Pxz_e\",\"Pyz_e\"],                     # what we want to predict\n",
    "                                'choose_species' :    ['e',None],   # which species to take\n",
    "                                'choose_x' : [0,64], 'choose_y' : [0,64], 'verbose' : False  # which part of the domain to take. The full domain is 2048x2048 and is too heavy for the tutorial\n",
    "                          } # make sure not to put a comma here, otherwise the object becomes a tuple rather than a dictionary which causes all sorts of problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ae088",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_kwargs = {'data_folder': \"/volume1/scratch/share_dir/nn/data/raw_data/\",               # where the data is stored\n",
    "                          'image_file_name_column' : 'filenames',                       # the column in the csv file that contains the image file names\n",
    "                          'read_features_targets_kwargs' : read_features_targets_kwargs,\n",
    "                          'train_sample' :  \"/volume1/scratch/share_dir/ecsim/sampling/peppe/intraexp/train.csv\",  # this should be the path to the csv file that contains a list of training samples, each cell corresponds to local paths to the actual data relative to `data_folder`\n",
    "                          'val_sample' :    \"/volume1/scratch/share_dir/ecsim/sampling/peppe/intraexp/val.csv\",    # same but validation\n",
    "                          'test_sample' :   \"/volume1/scratch/share_dir/ecsim/sampling/peppe/intraexp/test.csv\",  # same but testing\n",
    "                          'scaler_features' : False, #True,    # Whether to normalize the features. It is called scaler because it rescales to 0 mean and 1 variance\n",
    "                          'scaler_targets' : False, #True,     # Whether to normalize the targets\n",
    "                          'prescaler_targets' : None #['log','log','log',None,None,None]  # If we want to transform the targets before training. The order in which they appear correspond to `request_targets` in `read_features_targets_kwargs``\n",
    "                      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80e18973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filenames</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>high_res_hbg/DoubleHarris-Fields_011000.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high_res_hbg/DoubleHarris-Fields_014000.h5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>high_res_hbg/DoubleHarris-Fields_019000.h5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    filenames\n",
       "0  high_res_hbg/DoubleHarris-Fields_011000.h5\n",
       "1  high_res_hbg/DoubleHarris-Fields_014000.h5\n",
       "2  high_res_hbg/DoubleHarris-Fields_019000.h5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(dataset_kwargs['test_sample'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b12cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_kwargs={'train_loader_kwargs' : {'batch_size' : 512,      #training batch size\n",
    "                                           'subsample_rate' : .2,   #subsample rate which defines a fraction of data that is used when sampling. 1 means all data is used\n",
    "                                           'subsample_seed' : 42,   #seed for the subsampling\n",
    "                                           'seed' : 42,             #seed for the random number generator\n",
    "                                           'shuffle' : True},       #shuffle the training set\n",
    "                    'val_loader_kwargs' : {'batch_size' : 512,        #validation batch size\n",
    "                                           'subsample_rate' : .2, \n",
    "                                           'subsample_seed' : 42, \n",
    "                                           'shuffle' : False}}      # we don't need to shuffle the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1c7139f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kwargs={'model': 'MLP',    # the model class name we want to use, check `src/models.py` for available models\n",
    "              'model_seed' : 42, # seed for the random initialization of the model\n",
    "              'feature_dims' : [10, 60, 80, 50, 40, 6], # the dimensions of the layers in the model. The first element should be the number of `request_features` in `read_features_targets_kwargs` and the last element should be the number of `request_targets` in `read_features_targets_kwargs`\n",
    "              'activations' : ['Tanh','Tanh','ReLU','Tanh', None], \n",
    "    'optimizer_kwargs': {'optimizer' : 'Adam',      # look up https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#adam \n",
    "                         'lr' : 5e-4,               # learning rate\n",
    "                         'criterion': 'MSELoss'},   # the loss function to optimize\n",
    "    'scheduler_kwargs': {'scheduler' : 'ReduceLROnPlateau',  # look up https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html\n",
    "                         'mode' : 'min',  # the remainder are the arguments for the ReduceLROnPlateau scheduler\n",
    "                         'factor' : 0.2, \n",
    "                         'patience' : 10, \n",
    "                         'cooldown' : 15 , \n",
    "                         'epochs' : 10,                 # number of epochs before stoppig training\n",
    "                         'early_stopping' : 20},        # number of epochs to tolarate without \n",
    "    'logger_kwargs':{'update_step': 1, 'show': True}}   # some details about how to log the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a5364bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 16:07:04,187 | INFO | src.trainers | \t Creating a new configuration file: /volume1/scratch/georgem/closure/dev/dev16/config.json\n",
      "2025-07-14 16:07:04,188 | INFO | src.trainers | \t  \n",
      "2025-07-14 16:07:04,189 | INFO | src.trainers | \t ===Logging to /volume1/scratch/georgem/closure/dev/dev16//training.log on level 20, @ self.rank=None, self.local_rank=None, self.device=device(type='cpu') ===\n",
      "2025-07-14 16:07:04,189 | INFO | src.trainers | \t host: haydn.esat.kuleuven.be\n",
      "2025-07-14 16:07:04,190 | INFO | src.trainers | \t  \n",
      "2025-07-14 16:07:04,191 | INFO | src.datasets | \t  This is train set\n",
      "2025-07-14 16:07:04,191 | INFO | src.datasets | \t  This is train set\n",
      "2025-07-14 16:07:04,192 | INFO | src.datasets | \t Datasplit performed according to /volume1/scratch/share_dir/ecsim/sampling/peppe/intraexp/train.csv\n",
      "2025-07-14 16:07:04,192 | INFO | src.datasets | \t Datasplit performed according to /volume1/scratch/share_dir/ecsim/sampling/peppe/intraexp/train.csv\n",
      "2025-07-14 16:07:05,986 | INFO | src.datasets | \t Features shape: (32768, 10), Targets shape: (32768, 6)\n",
      "2025-07-14 16:07:05,986 | INFO | src.datasets | \t Features shape: (32768, 10), Targets shape: (32768, 6)\n",
      "2025-07-14 16:07:05,990 | INFO | src.datasets | \t Saved self.features_mean, self.features_std to /volume1/scratch/georgem/closure/dev/dev16//X.pkl\n",
      "2025-07-14 16:07:05,990 | INFO | src.datasets | \t Saved self.features_mean, self.features_std to /volume1/scratch/georgem/closure/dev/dev16//X.pkl\n",
      "2025-07-14 16:07:05,991 | INFO | src.datasets | \t Normalization applied to features\n",
      "2025-07-14 16:07:05,991 | INFO | src.datasets | \t Normalization applied to features\n",
      "2025-07-14 16:07:05,994 | WARNING | py.warnings | \t /volume1/scratch/georgem/closure/examples/../src/datasets.py:711: RuntimeWarning: invalid value encountered in log\n",
      "  self.targets[:,channel,...] = self.prescaler_targets[channel](self.targets[:,channel,...])\n",
      "\n",
      "2025-07-14 16:07:05,994 | WARNING | py.warnings | \t /volume1/scratch/georgem/closure/examples/../src/datasets.py:711: RuntimeWarning: invalid value encountered in log\n",
      "  self.targets[:,channel,...] = self.prescaler_targets[channel](self.targets[:,channel,...])\n",
      "\n",
      "2025-07-14 16:07:05,994 | WARNING | py.warnings | \t /volume1/scratch/georgem/closure/examples/../src/datasets.py:711: RuntimeWarning: invalid value encountered in log\n",
      "  self.targets[:,channel,...] = self.prescaler_targets[channel](self.targets[:,channel,...])\n",
      "\n",
      "2025-07-14 16:07:05,994 | WARNING | py.warnings | \t /volume1/scratch/georgem/closure/examples/../src/datasets.py:711: RuntimeWarning: invalid value encountered in log\n",
      "  self.targets[:,channel,...] = self.prescaler_targets[channel](self.targets[:,channel,...])\n",
      "\n",
      "2025-07-14 16:07:05,995 | INFO | src.datasets | \t Prescaling <ufunc 'log'> applied to targets\n",
      "2025-07-14 16:07:05,995 | INFO | src.datasets | \t Prescaling <ufunc 'log'> applied to targets\n",
      "2025-07-14 16:07:05,996 | INFO | src.datasets | \t Prescaling <ufunc 'log'> applied to targets\n",
      "2025-07-14 16:07:05,996 | INFO | src.datasets | \t Prescaling <ufunc 'log'> applied to targets\n",
      "2025-07-14 16:07:05,996 | INFO | src.datasets | \t Prescaling <ufunc 'log'> applied to targets\n",
      "2025-07-14 16:07:05,996 | INFO | src.datasets | \t Prescaling <ufunc 'log'> applied to targets\n",
      "2025-07-14 16:07:05,998 | INFO | src.datasets | \t Saved self.targets_mean, self.targets_std to /volume1/scratch/georgem/closure/dev/dev16//y.pkl\n",
      "2025-07-14 16:07:05,998 | INFO | src.datasets | \t Saved self.targets_mean, self.targets_std to /volume1/scratch/georgem/closure/dev/dev16//y.pkl\n",
      "2025-07-14 16:07:05,998 | INFO | src.datasets | \t Normalization applied to targets\n",
      "2025-07-14 16:07:05,998 | INFO | src.datasets | \t Normalization applied to targets\n",
      "2025-07-14 16:07:06,000 | INFO | src.datasets | \t  This is val set\n",
      "2025-07-14 16:07:06,000 | INFO | src.datasets | \t  This is val set\n",
      "2025-07-14 16:07:06,001 | INFO | src.datasets | \t Datasplit performed according to /volume1/scratch/share_dir/ecsim/sampling/peppe/intraexp/val.csv\n",
      "2025-07-14 16:07:06,001 | INFO | src.datasets | \t Datasplit performed according to /volume1/scratch/share_dir/ecsim/sampling/peppe/intraexp/val.csv\n",
      "2025-07-14 16:07:06,894 | INFO | src.datasets | \t Features shape: (16384, 10), Targets shape: (16384, 6)\n",
      "2025-07-14 16:07:06,894 | INFO | src.datasets | \t Features shape: (16384, 10), Targets shape: (16384, 6)\n",
      "2025-07-14 16:07:06,896 | INFO | src.datasets | \t Loaded self.features_mean, self.features_std from /volume1/scratch/georgem/closure/dev/dev16//X.pkl\n",
      "2025-07-14 16:07:06,896 | INFO | src.datasets | \t Loaded self.features_mean, self.features_std from /volume1/scratch/georgem/closure/dev/dev16//X.pkl\n",
      "2025-07-14 16:07:06,896 | INFO | src.datasets | \t Normalization applied to features\n",
      "2025-07-14 16:07:06,896 | INFO | src.datasets | \t Normalization applied to features\n",
      "2025-07-14 16:07:06,897 | WARNING | py.warnings | \t /volume1/scratch/georgem/closure/examples/../src/datasets.py:711: RuntimeWarning: invalid value encountered in log\n",
      "  self.targets[:,channel,...] = self.prescaler_targets[channel](self.targets[:,channel,...])\n",
      "\n",
      "2025-07-14 16:07:06,897 | WARNING | py.warnings | \t /volume1/scratch/georgem/closure/examples/../src/datasets.py:711: RuntimeWarning: invalid value encountered in log\n",
      "  self.targets[:,channel,...] = self.prescaler_targets[channel](self.targets[:,channel,...])\n",
      "\n",
      "2025-07-14 16:07:06,897 | WARNING | py.warnings | \t /volume1/scratch/georgem/closure/examples/../src/datasets.py:711: RuntimeWarning: invalid value encountered in log\n",
      "  self.targets[:,channel,...] = self.prescaler_targets[channel](self.targets[:,channel,...])\n",
      "\n",
      "2025-07-14 16:07:06,897 | WARNING | py.warnings | \t /volume1/scratch/georgem/closure/examples/../src/datasets.py:711: RuntimeWarning: invalid value encountered in log\n",
      "  self.targets[:,channel,...] = self.prescaler_targets[channel](self.targets[:,channel,...])\n",
      "\n",
      "2025-07-14 16:07:06,898 | INFO | src.datasets | \t Prescaling <ufunc 'log'> applied to targets\n",
      "2025-07-14 16:07:06,898 | INFO | src.datasets | \t Prescaling <ufunc 'log'> applied to targets\n",
      "2025-07-14 16:07:06,899 | INFO | src.datasets | \t Prescaling <ufunc 'log'> applied to targets\n",
      "2025-07-14 16:07:06,899 | INFO | src.datasets | \t Prescaling <ufunc 'log'> applied to targets\n",
      "2025-07-14 16:07:06,899 | INFO | src.datasets | \t Prescaling <ufunc 'log'> applied to targets\n",
      "2025-07-14 16:07:06,899 | INFO | src.datasets | \t Prescaling <ufunc 'log'> applied to targets\n",
      "2025-07-14 16:07:06,900 | INFO | src.datasets | \t Loaded self.targets_mean, self.targets_std from /volume1/scratch/georgem/closure/dev/dev16//y.pkl\n",
      "2025-07-14 16:07:06,900 | INFO | src.datasets | \t Loaded self.targets_mean, self.targets_std from /volume1/scratch/georgem/closure/dev/dev16//y.pkl\n",
      "2025-07-14 16:07:06,900 | INFO | src.datasets | \t Normalization applied to targets\n",
      "2025-07-14 16:07:06,900 | INFO | src.datasets | \t Normalization applied to targets\n",
      "2025-07-14 16:07:06,901 | INFO | src.datasets | \t  This is test set\n",
      "2025-07-14 16:07:06,901 | INFO | src.datasets | \t  This is test set\n",
      "2025-07-14 16:07:06,902 | INFO | src.datasets | \t Datasplit performed according to /volume1/scratch/share_dir/ecsim/sampling/peppe/intraexp/test.csv\n",
      "2025-07-14 16:07:06,902 | INFO | src.datasets | \t Datasplit performed according to /volume1/scratch/share_dir/ecsim/sampling/peppe/intraexp/test.csv\n",
      "2025-07-14 16:07:07,544 | INFO | src.datasets | \t Features shape: (12288, 10), Targets shape: (12288, 6)\n",
      "2025-07-14 16:07:07,544 | INFO | src.datasets | \t Features shape: (12288, 10), Targets shape: (12288, 6)\n",
      "2025-07-14 16:07:07,546 | INFO | src.datasets | \t Loaded self.features_mean, self.features_std from /volume1/scratch/georgem/closure/dev/dev16//X.pkl\n",
      "2025-07-14 16:07:07,546 | INFO | src.datasets | \t Loaded self.features_mean, self.features_std from /volume1/scratch/georgem/closure/dev/dev16//X.pkl\n",
      "2025-07-14 16:07:07,547 | INFO | src.datasets | \t Normalization applied to features\n",
      "2025-07-14 16:07:07,547 | INFO | src.datasets | \t Normalization applied to features\n",
      "2025-07-14 16:07:07,548 | WARNING | py.warnings | \t /volume1/scratch/georgem/closure/examples/../src/datasets.py:711: RuntimeWarning: invalid value encountered in log\n",
      "  self.targets[:,channel,...] = self.prescaler_targets[channel](self.targets[:,channel,...])\n",
      "\n",
      "2025-07-14 16:07:07,548 | WARNING | py.warnings | \t /volume1/scratch/georgem/closure/examples/../src/datasets.py:711: RuntimeWarning: invalid value encountered in log\n",
      "  self.targets[:,channel,...] = self.prescaler_targets[channel](self.targets[:,channel,...])\n",
      "\n",
      "2025-07-14 16:07:07,548 | WARNING | py.warnings | \t /volume1/scratch/georgem/closure/examples/../src/datasets.py:711: RuntimeWarning: invalid value encountered in log\n",
      "  self.targets[:,channel,...] = self.prescaler_targets[channel](self.targets[:,channel,...])\n",
      "\n",
      "2025-07-14 16:07:07,548 | WARNING | py.warnings | \t /volume1/scratch/georgem/closure/examples/../src/datasets.py:711: RuntimeWarning: invalid value encountered in log\n",
      "  self.targets[:,channel,...] = self.prescaler_targets[channel](self.targets[:,channel,...])\n",
      "\n",
      "2025-07-14 16:07:07,549 | INFO | src.datasets | \t Prescaling <ufunc 'log'> applied to targets\n",
      "2025-07-14 16:07:07,549 | INFO | src.datasets | \t Prescaling <ufunc 'log'> applied to targets\n",
      "2025-07-14 16:07:07,549 | INFO | src.datasets | \t Prescaling <ufunc 'log'> applied to targets\n",
      "2025-07-14 16:07:07,549 | INFO | src.datasets | \t Prescaling <ufunc 'log'> applied to targets\n",
      "2025-07-14 16:07:07,550 | INFO | src.datasets | \t Prescaling <ufunc 'log'> applied to targets\n",
      "2025-07-14 16:07:07,550 | INFO | src.datasets | \t Prescaling <ufunc 'log'> applied to targets\n",
      "2025-07-14 16:07:07,550 | INFO | src.datasets | \t Loaded self.targets_mean, self.targets_std from /volume1/scratch/georgem/closure/dev/dev16//y.pkl\n",
      "2025-07-14 16:07:07,550 | INFO | src.datasets | \t Loaded self.targets_mean, self.targets_std from /volume1/scratch/georgem/closure/dev/dev16//y.pkl\n",
      "2025-07-14 16:07:07,551 | INFO | src.datasets | \t Normalization applied to targets\n",
      "2025-07-14 16:07:07,551 | INFO | src.datasets | \t Normalization applied to targets\n",
      "2025-07-14 16:07:07,552 | INFO | src.trainers | \t Comprehending the configuration settings\n",
      "2025-07-14 16:07:07,552 | INFO | src.trainers | \t Comprehending the configuration settings\n",
      "2025-07-14 16:07:07,553 | WARNING | src.trainers | \t Creating new model. Note this will replace any previous model\n",
      "2025-07-14 16:07:07,553 | WARNING | src.trainers | \t Creating new model. Note this will replace any previous model\n",
      "2025-07-14 16:07:07,553 | INFO | src.models | \t torch.cuda.is_available() = False\n",
      "2025-07-14 16:07:07,553 | INFO | src.models | \t torch.cuda.is_available() = False\n",
      "2025-07-14 16:07:07,555 | INFO | src.models | \t Initializing model MLP(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=60, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=60, out_features=80, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=80, out_features=50, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=40, bias=True)\n",
      "    (7): Tanh()\n",
      "    (8): Linear(in_features=40, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "2025-07-14 16:07:07,555 | INFO | src.models | \t Initializing model MLP(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=60, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=60, out_features=80, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=80, out_features=50, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=40, bias=True)\n",
      "    (7): Tanh()\n",
      "    (8): Linear(in_features=40, out_features=6, bias=True)\n",
      "  )\n",
      ")\n",
      "2025-07-14 16:07:07,556 | INFO | src.models | \t DDP not available, initializing model using single GPU self.local_rank = None\n",
      "2025-07-14 16:07:07,556 | INFO | src.models | \t DDP not available, initializing model using single GPU self.local_rank = None\n",
      "2025-07-14 16:07:07,565 | INFO | src.models | \t Optimization criterion MSELoss()\n",
      "2025-07-14 16:07:07,565 | INFO | src.models | \t Optimization criterion MSELoss()\n",
      "2025-07-14 16:07:07,566 | INFO | src.models | \t Creating object: <src.models.PyNet object at 0x7f4805578c50> which contains MLP(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=60, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=60, out_features=80, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=80, out_features=50, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=40, bias=True)\n",
      "    (7): Tanh()\n",
      "    (8): Linear(in_features=40, out_features=6, bias=True)\n",
      "  )\n",
      ") as the model\n",
      "2025-07-14 16:07:07,566 | INFO | src.models | \t Creating object: <src.models.PyNet object at 0x7f4805578c50> which contains MLP(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=60, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=60, out_features=80, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=80, out_features=50, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=50, out_features=40, bias=True)\n",
      "    (7): Tanh()\n",
      "    (8): Linear(in_features=40, out_features=6, bias=True)\n",
      "  )\n",
      ") as the model\n",
      "2025-07-14 16:07:07,567 | INFO | src.trainers | \t self.device = device(type='cpu') on self.local_rank = None\n",
      "2025-07-14 16:07:07,567 | INFO | src.trainers | \t self.device = device(type='cpu') on self.local_rank = None\n",
      "2025-07-14 16:07:07,571 | INFO | src.trainers | \t Code version git hash: 665aa52b25ae6b4e7ecaa3517b8d159556d0471a\n",
      "2025-07-14 16:07:07,571 | INFO | src.trainers | \t Code version git hash: 665aa52b25ae6b4e7ecaa3517b8d159556d0471a\n",
      "2025-07-14 16:07:07,572 | INFO | src.trainers | \t Creating data loaders with sampler_type = 'serial' because self.world_size = None\n",
      "2025-07-14 16:07:07,572 | INFO | src.trainers | \t Creating data loaders with sampler_type = 'serial' because self.world_size = None\n",
      "2025-07-14 16:07:07,573 | INFO | src.datasets | \t ChannelDataLoader.feature_channels: None\n",
      "2025-07-14 16:07:07,573 | INFO | src.datasets | \t ChannelDataLoader.feature_channels: None\n",
      "2025-07-14 16:07:07,573 | INFO | src.datasets | \t ChannelDataLoader.target_channels: None\n",
      "2025-07-14 16:07:07,573 | INFO | src.datasets | \t ChannelDataLoader.target_channels: None\n",
      "2025-07-14 16:07:07,574 | INFO | src.datasets | \t 32768, len(dataset.targets) = 32768 samples before subsampling\n",
      "2025-07-14 16:07:07,574 | INFO | src.datasets | \t 32768, len(dataset.targets) = 32768 samples before subsampling\n",
      "2025-07-14 16:07:07,575 | INFO | src.datasets | \t len(self.subset) = 6553 samples after subsampling\n",
      "2025-07-14 16:07:07,575 | INFO | src.datasets | \t len(self.subset) = 6553 samples after subsampling\n",
      "2025-07-14 16:07:07,576 | INFO | src.datasets | \t ChannelDataLoader.feature_channels: None\n",
      "2025-07-14 16:07:07,576 | INFO | src.datasets | \t ChannelDataLoader.feature_channels: None\n",
      "2025-07-14 16:07:07,576 | INFO | src.datasets | \t ChannelDataLoader.target_channels: None\n",
      "2025-07-14 16:07:07,576 | INFO | src.datasets | \t ChannelDataLoader.target_channels: None\n",
      "2025-07-14 16:07:07,577 | INFO | src.datasets | \t 16384, len(dataset.targets) = 16384 samples before subsampling\n",
      "2025-07-14 16:07:07,577 | INFO | src.datasets | \t 16384, len(dataset.targets) = 16384 samples before subsampling\n",
      "2025-07-14 16:07:07,577 | INFO | src.datasets | \t len(self.subset) = 3276 samples after subsampling\n",
      "2025-07-14 16:07:07,577 | INFO | src.datasets | \t len(self.subset) = 3276 samples after subsampling\n",
      "2025-07-14 16:07:07,578 | INFO | src.datasets | \t ChannelDataLoader.feature_channels: None\n",
      "2025-07-14 16:07:07,578 | INFO | src.datasets | \t ChannelDataLoader.feature_channels: None\n",
      "2025-07-14 16:07:07,578 | INFO | src.datasets | \t ChannelDataLoader.target_channels: None\n",
      "2025-07-14 16:07:07,578 | INFO | src.datasets | \t ChannelDataLoader.target_channels: None\n",
      "2025-07-14 16:07:07,579 | INFO | src.datasets | \t 12288, len(dataset.targets) = 12288 samples before subsampling\n",
      "2025-07-14 16:07:07,579 | INFO | src.datasets | \t 12288, len(dataset.targets) = 12288 samples before subsampling\n",
      "2025-07-14 16:07:07,580 | INFO | src.datasets | \t len(self.subset) = 2457 samples after subsampling\n",
      "2025-07-14 16:07:07,580 | INFO | src.datasets | \t len(self.subset) = 2457 samples after subsampling\n"
     ]
    }
   ],
   "source": [
    "work_dir = '/volume1/scratch/georgem/closure/dev/dev16/'  # you may have to create containing path before running the script (check in the source code). It is better to use absolute paths than relative paths for the operations below to work\n",
    "                                                                            # the /dev16/ folder on the other hand will be definitely created by the trainer so you don't need to create it\n",
    "! rm -rf {work_dir}  # This will delete the previously existing model\n",
    "trainer = tr.Trainer(work_dir=work_dir,   # where the model will be saved\n",
    "                     dataset_kwargs=dataset_kwargs, load_data_kwargs=load_data_kwargs, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "385a8b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9802e-09)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.mean(trainer.train_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c2c2cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json  training.log  X.pkl  y.pkl\n"
     ]
    }
   ],
   "source": [
    "! ls {work_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9296d358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer work dir: \t\t/volume1/scratch/georgem/closure/dev/dev16/\n",
      "trainer class instance: \t<src.trainers.Trainer object at 0x7f4805563a10>\n",
      "trainer loader: \t\t<src.datasets.ChannelDataLoader object at 0x7f480592c920>\n",
      "train dataset features shape: \t torch.Size([32768, 10])\n",
      "train dataset targets shape: \ttorch.Size([32768, 6])\n"
     ]
    }
   ],
   "source": [
    "print(f\"trainer work dir: \\t\\t{trainer.work_dir}\")\n",
    "print(f\"trainer class instance: \\t{trainer}\")\n",
    "print(f\"trainer loader: \\t\\t{trainer.train_loader}\")\n",
    "print(f\"train dataset features shape: \\t {trainer.train_dataset.features.shape}\")\n",
    "print(f\"train dataset targets shape: \\t{trainer.train_dataset.targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba80c2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 16:08:08,170 | INFO | src.trainers | \t Prior to fit: RAM memory % used: 3.8, RAM Used (GB):, 8.257073152, process RAM usage (GB): 0.6456718444824219\n",
      "2025-07-14 16:08:08,170 | INFO | src.trainers | \t Prior to fit: RAM memory % used: 3.8, RAM Used (GB):, 8.257073152, process RAM usage (GB): 0.6456718444824219\n",
      "2025-07-14 16:08:08,172 | INFO | src.models | \t Each forward pass had 13 train batches.\n",
      "2025-07-14 16:08:08,172 | INFO | src.models | \t Each forward pass had 13 train batches.\n",
      "2025-07-14 16:08:09,719 | INFO | src.models | \t Number of samples per batch: len(next(iter(train_loader))[0]) = 512\n",
      "2025-07-14 16:08:09,719 | INFO | src.models | \t Number of samples per batch: len(next(iter(train_loader))[0]) = 512\n",
      "2025-07-14 16:08:14,419 | INFO | src.models | \t Epoch 1/10 | Train loss: nan | Val loss: nan | Time/epoch: 4.698 s | Time/epoch_train: 3.209 s | Learn rate: [0.0005]\n",
      "2025-07-14 16:08:14,419 | INFO | src.models | \t Epoch 1/10 | Train loss: nan | Val loss: nan | Time/epoch: 4.698 s | Time/epoch_train: 3.209 s | Learn rate: [0.0005]\n",
      "2025-07-14 16:08:17,299 | INFO | src.models | \t Epoch 2/10 | Train loss: nan | Val loss: nan | Time/epoch: 2.879 s | Time/epoch_train: 1.363 s | Learn rate: [0.0005]\n",
      "2025-07-14 16:08:17,299 | INFO | src.models | \t Epoch 2/10 | Train loss: nan | Val loss: nan | Time/epoch: 2.879 s | Time/epoch_train: 1.363 s | Learn rate: [0.0005]\n",
      "2025-07-14 16:08:20,344 | INFO | src.models | \t Epoch 3/10 | Train loss: nan | Val loss: nan | Time/epoch: 3.043 s | Time/epoch_train: 1.543 s | Learn rate: [0.0005]\n",
      "2025-07-14 16:08:20,344 | INFO | src.models | \t Epoch 3/10 | Train loss: nan | Val loss: nan | Time/epoch: 3.043 s | Time/epoch_train: 1.543 s | Learn rate: [0.0005]\n",
      "2025-07-14 16:08:23,649 | INFO | src.models | \t Epoch 4/10 | Train loss: nan | Val loss: nan | Time/epoch: 3.303 s | Time/epoch_train: 1.676 s | Learn rate: [0.0005]\n",
      "2025-07-14 16:08:23,649 | INFO | src.models | \t Epoch 4/10 | Train loss: nan | Val loss: nan | Time/epoch: 3.303 s | Time/epoch_train: 1.676 s | Learn rate: [0.0005]\n",
      "2025-07-14 16:08:26,666 | INFO | src.models | \t Epoch 5/10 | Train loss: nan | Val loss: nan | Time/epoch: 3.015 s | Time/epoch_train: 1.635 s | Learn rate: [0.0005]\n",
      "2025-07-14 16:08:26,666 | INFO | src.models | \t Epoch 5/10 | Train loss: nan | Val loss: nan | Time/epoch: 3.015 s | Time/epoch_train: 1.635 s | Learn rate: [0.0005]\n",
      "2025-07-14 16:08:29,873 | INFO | src.models | \t Epoch 6/10 | Train loss: nan | Val loss: nan | Time/epoch: 3.205 s | Time/epoch_train: 1.581 s | Learn rate: [0.0005]\n",
      "2025-07-14 16:08:29,873 | INFO | src.models | \t Epoch 6/10 | Train loss: nan | Val loss: nan | Time/epoch: 3.205 s | Time/epoch_train: 1.581 s | Learn rate: [0.0005]\n",
      "2025-07-14 16:08:32,964 | INFO | src.models | \t Epoch 7/10 | Train loss: nan | Val loss: nan | Time/epoch: 3.09 s | Time/epoch_train: 1.596 s | Learn rate: [0.0005]\n",
      "2025-07-14 16:08:32,964 | INFO | src.models | \t Epoch 7/10 | Train loss: nan | Val loss: nan | Time/epoch: 3.09 s | Time/epoch_train: 1.596 s | Learn rate: [0.0005]\n",
      "2025-07-14 16:08:35,962 | INFO | src.models | \t Epoch 8/10 | Train loss: nan | Val loss: nan | Time/epoch: 2.996 s | Time/epoch_train: 1.612 s | Learn rate: [0.0005]\n",
      "2025-07-14 16:08:35,962 | INFO | src.models | \t Epoch 8/10 | Train loss: nan | Val loss: nan | Time/epoch: 2.996 s | Time/epoch_train: 1.612 s | Learn rate: [0.0005]\n",
      "2025-07-14 16:08:39,149 | INFO | src.models | \t Epoch 9/10 | Train loss: nan | Val loss: nan | Time/epoch: 3.185 s | Time/epoch_train: 1.484 s | Learn rate: [0.0005]\n",
      "2025-07-14 16:08:39,149 | INFO | src.models | \t Epoch 9/10 | Train loss: nan | Val loss: nan | Time/epoch: 3.185 s | Time/epoch_train: 1.484 s | Learn rate: [0.0005]\n",
      "2025-07-14 16:08:42,165 | INFO | src.models | \t Epoch 10/10 | Train loss: nan | Val loss: nan | Time/epoch: 3.015 s | Time/epoch_train: 1.504 s | Learn rate: [0.0005]\n",
      "2025-07-14 16:08:42,165 | INFO | src.models | \t Epoch 10/10 | Train loss: nan | Val loss: nan | Time/epoch: 3.015 s | Time/epoch_train: 1.504 s | Learn rate: [0.0005]\n",
      "2025-07-14 16:08:42,167 | INFO | src.models | \t Each forward pass had 13 train batches and forward pass had final self.train_batch_idx = 12.\n",
      "2025-07-14 16:08:42,167 | INFO | src.models | \t Each forward pass had 13 train batches and forward pass had final self.train_batch_idx = 12.\n",
      "2025-07-14 16:08:43,683 | INFO | src.models | \t Number of samples per batch: len(next(iter(train_loader))[0]) = 512\n",
      "2025-07-14 16:08:43,683 | INFO | src.models | \t Number of samples per batch: len(next(iter(train_loader))[0]) = 512\n",
      "2025-07-14 16:08:43,685 | INFO | src.models | \t End of training on | self.rank = None, self.device = device(type='cpu'). Total time: 33.9954 seconds\n",
      "2025-07-14 16:08:43,685 | INFO | src.models | \t End of training on | self.rank = None, self.device = device(type='cpu'). Total time: 33.9954 seconds\n",
      "2025-07-14 16:08:43,686 | INFO | src.trainers | \t After fit: RAM memory % used: 3.6, RAM Used (GB):, 7.776002048, process RAM usage (GB): 0.647613525390625\n",
      "2025-07-14 16:08:43,686 | INFO | src.trainers | \t After fit: RAM memory % used: 3.6, RAM Used (GB):, 7.776002048, process RAM usage (GB): 0.647613525390625\n",
      "2025-07-14 16:08:43,686 | INFO | src.trainers | \t Saving the model weights and loss history to /volume1/scratch/georgem/closure/dev/dev16///\n",
      "2025-07-14 16:08:43,686 | INFO | src.trainers | \t Saving the model weights and loss history to /volume1/scratch/georgem/closure/dev/dev16///\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e426a958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32768, 6])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train_dataset.targets.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
